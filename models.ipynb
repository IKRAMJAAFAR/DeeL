{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assume df contains the full dataset with the 'timestamp' column and 'value' column\n",
    "url = \"https://raw.githubusercontent.com/IKRAMJAAFAR/DeeL/refs/heads/main/dataset.csv?token=GHSAT0AAAAAACZZJM4A75DJZ7QS6RRPDJOOZZDRB5A\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Extract the target variable (passenger count) and features (timestamp or time-related features)\n",
    "values = df['value'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the values for better LSTM training\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "values_scaled = scaler.fit_transform(values)\n",
    "\n",
    "# Function to create sequences of data for LSTM input\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length, 0])\n",
    "        y.append(data[i+sequence_length, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define the sequence length (number of previous time steps to consider for prediction)\n",
    "sequence_length = 30 \n",
    "\n",
    "# Create sequences of data\n",
    "X, y = create_sequences(values_scaled, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Reshape X_train and X_test for LSTM (samples, time steps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 3s 15ms/step - loss: 0.0311 - val_loss: 0.0063\n",
      "65/65 [==============================] - 0s 4ms/step\n",
      "Preliminary MSE: 0.006273112107557844\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))  # LSTM layer\n",
    "model.add(Dense(1))  # Output layer for regression\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model without hyperparameter tuning (Preliminary)\n",
    "model.fit(X_train, y_train, epochs=1, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute preliminary evaluation metrics (MSE for this example)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Preliminary MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "129/129 [==============================] - 7s 36ms/step - loss: 0.0405 - val_loss: 0.0236\n",
      "Epoch 2/10\n",
      "129/129 [==============================] - 4s 29ms/step - loss: 0.0185 - val_loss: 0.0123\n",
      "Epoch 3/10\n",
      "129/129 [==============================] - 4s 29ms/step - loss: 0.0146 - val_loss: 0.0064\n",
      "Epoch 4/10\n",
      "129/129 [==============================] - 4s 30ms/step - loss: 0.0045 - val_loss: 0.0058\n",
      "Epoch 5/10\n",
      "129/129 [==============================] - 4s 33ms/step - loss: 0.0030 - val_loss: 0.0046\n",
      "Epoch 6/10\n",
      "129/129 [==============================] - 4s 32ms/step - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 7/10\n",
      "129/129 [==============================] - 4s 32ms/step - loss: 0.0023 - val_loss: 0.0033\n",
      "Epoch 8/10\n",
      "129/129 [==============================] - 4s 31ms/step - loss: 0.0021 - val_loss: 0.0034\n",
      "Epoch 9/10\n",
      "129/129 [==============================] - 4s 31ms/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 10/10\n",
      "129/129 [==============================] - 4s 31ms/step - loss: 0.0019 - val_loss: 0.0026\n",
      "65/65 [==============================] - 1s 9ms/step\n",
      "Anomalies detected: 103\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed\n",
    "\n",
    "# Define LSTM Autoencoder\n",
    "model = Sequential()\n",
    "# Encoder part\n",
    "model.add(LSTM(64, activation='relu', input_shape=(X_train.shape[1], 1), return_sequences=False))\n",
    "model.add(RepeatVector(X_train.shape[1]))\n",
    "# Decoder part\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "model.fit(X_train, X_train, epochs=10, batch_size=64, validation_data=(X_test, X_test))\n",
    "\n",
    "# Predict and compute reconstruction error\n",
    "reconstruction = model.predict(X_test)\n",
    "reconstruction_error = np.mean(np.abs(reconstruction - X_test), axis=1)\n",
    "\n",
    "# Set threshold for anomaly detection\n",
    "threshold = np.percentile(reconstruction_error, 95)\n",
    "anomalies = reconstruction_error > threshold\n",
    "print(f\"Anomalies detected: {np.sum(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "129/129 [==============================] - 22s 138ms/step - loss: 0.0261\n",
      "Epoch 2/10\n",
      "129/129 [==============================] - 19s 145ms/step - loss: 0.0038\n",
      "Epoch 3/10\n",
      "129/129 [==============================] - 17s 133ms/step - loss: 0.0017\n",
      "Epoch 4/10\n",
      "129/129 [==============================] - 19s 146ms/step - loss: 6.8239e-04\n",
      "Epoch 5/10\n",
      "129/129 [==============================] - 17s 132ms/step - loss: 2.3346e-04\n",
      "Epoch 6/10\n",
      "129/129 [==============================] - 19s 147ms/step - loss: 8.9663e-05\n",
      "Epoch 7/10\n",
      "129/129 [==============================] - 17s 131ms/step - loss: 4.2086e-05\n",
      "Epoch 8/10\n",
      "129/129 [==============================] - 18s 141ms/step - loss: 3.2162e-05\n",
      "Epoch 9/10\n",
      "129/129 [==============================] - 18s 138ms/step - loss: 3.3162e-05\n",
      "Epoch 10/10\n",
      "129/129 [==============================] - 21s 163ms/step - loss: 3.3621e-05\n",
      "65/65 [==============================] - 3s 34ms/step\n",
      "Anomalies detected: 103\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "# Define ConvLSTM model\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same', input_shape=(sequence_length, 1, 1, 1), return_sequences=True))\n",
    "# Optional: Add more ConvLSTM layers if needed\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same', return_sequences=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))  # Regression output (for anomaly score or classification)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Reshape input data for ConvLSTM\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], sequence_length, 1, 1, 1))  # Shape (batch, timesteps, height, width, channels)\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], sequence_length, 1, 1, 1))  # Shape (batch, timesteps, height, width, channels)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, X_train[:, -1], epochs=10, batch_size=64)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(X_test_reshaped)\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error = np.mean(np.abs(X_test_reshaped[:, :, 0, 0, 0] - predictions), axis=1)\n",
    "\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = np.percentile(reconstruction_error, 95)\n",
    "anomalies = reconstruction_error > threshold\n",
    "print(f\"Anomalies detected: {np.sum(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies detected: 2830\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/IKRAMJAAFAR/DeeL/refs/heads/main/dataset.csv?token=GHSAT0AAAAAACZZJM4A75DJZ7QS6RRPDJOOZZDRB5A\"\n",
    "df = pd.read_csv(url)\n",
    "# Step 1: Convert the 'timestamp' column to datetime format\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Step 2: Set 'timestamp' as the index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Step 3: Ensure pandas understands the frequency of the time series\n",
    "# For 30-minute frequency, use '30T' (30 minutes)\n",
    "df = df.asfreq('30T')\n",
    "\n",
    "# Step 4: Check for missing values and fill them if necessary\n",
    "if df.isnull().any().any():\n",
    "    print(\"Missing values detected, filling with forward fill...\")\n",
    "    df = df.fillna(method='ffill')  # Or use 'bfill' or interpolation based on the context\n",
    "\n",
    "# Step 5: Apply seasonal decomposition\n",
    "decomposition = seasonal_decompose(df['value'], model='additive', period=48)  # Adjust 'period' based on your data\n",
    "\n",
    "# Get the residuals (this part helps with anomaly detection)\n",
    "residual = decomposition.resid\n",
    "\n",
    "# Set threshold for anomaly detection based on residuals\n",
    "threshold = residual.std()  # Use the standard deviation of residuals as a threshold\n",
    "anomalies = residual.abs() > threshold  # Detect anomalies\n",
    "\n",
    "# Count the anomalies\n",
    "num_anomalies = np.sum(anomalies)\n",
    "print(f'Anomalies detected: {num_anomalies}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies detected: 383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "\n",
    "# Assume X_train and X_test are already prepared (flattened if necessary)\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], -1))  # Flatten data if necessary\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "# Train an SVM model for anomaly detection\n",
    "svm = OneClassSVM(nu=0.05, kernel='rbf', gamma='auto')\n",
    "svm.fit(X_train_reshaped)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = svm.predict(X_test_reshaped)\n",
    "anomalies = predictions == -1  # SVM labels -1 as anomalies and 1 as normal\n",
    "\n",
    "# Print results\n",
    "print(f\"Anomalies detected: {np.sum(anomalies)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
